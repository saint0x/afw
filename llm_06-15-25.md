# Jockey Image

Generated: 06-15-2025 at 13:14:42

## Repository Structure

```
llm
â”‚   â”œâ”€â”€ handler.ts
â”‚   â”œâ”€â”€ providers
    â””â”€â”€ types.ts
```

## File: /Users/deepsaint/Desktop/symphony-sdk/src/llm/providers/base.ts

```ts
import { 
    LLMProvider, 
    LLMConfig, 
    LLMRequest, 
    LLMResponse,
    LLMFunctionDefinition
} from '../types';
import { ExecutionMetrics } from '../../types/sdk';
import { logger, LogCategory } from '../../utils/logger';
import { ConfigurationError, LLMError, ValidationError, ErrorCode } from '../../errors/index';

export interface ExtendedLLMConfig extends LLMConfig {
    topP?: number;
    frequencyPenalty?: number;
    presencePenalty?: number;
    timeout?: number;
}

export interface ExtendedLLMRequest extends LLMRequest {
    functions?: LLMFunctionDefinition[];
    functionCall?: {
        name: string;
        arguments: any;
    };
}

export interface ExtendedLLMResponse extends LLMResponse {
    metrics?: ExecutionMetrics & {
        tokenUsage: {
            input: number;
            output: number;
            total: number;
        };
    };
}

export abstract class BaseLLMProvider implements LLMProvider {
    name: string;
    protected config: ExtendedLLMConfig;
    public supportsStreaming: boolean = false;
    protected supportsFunctions: boolean = false;
    protected supportsVision: boolean = false;

    constructor(config: ExtendedLLMConfig) {
        if (!config.provider) {
            throw new ConfigurationError(
                'Provider name is required',
                { config },
                { component: 'BaseLLMProvider', operation: 'constructor' }
            );
        }
        this.name = config.provider;
        this.config = {
            ...config,
            temperature: config.temperature ?? 0.7,
            maxTokens: config.maxTokens ?? 1000,
            topP: config.topP ?? 1,
            frequencyPenalty: config.frequencyPenalty ?? 0,
            presencePenalty: config.presencePenalty ?? 0,
            timeout: config.timeout ?? 30000
        };
    }

    abstract initialize(): Promise<void>;
    abstract complete(request: LLMRequest): Promise<LLMResponse>;
    abstract completeStream(request: LLMRequest): AsyncIterable<LLMResponse>;

    protected validateApiKey(): void {
        if (!this.config.apiKey) {
            throw new LLMError(
                ErrorCode.MISSING_API_KEY,
                `API key not provided for ${this.name} provider`,
                { provider: this.name },
                { component: 'BaseLLMProvider', operation: 'validateApiKey' }
            );
        }
        if (typeof this.config.apiKey !== 'string') {
            throw new ValidationError(
                `Invalid API key type for ${this.name} provider`,
                { 
                    provider: this.name, 
                    apiKeyType: typeof this.config.apiKey,
                    expectedType: 'string'
                },
                { component: 'BaseLLMProvider', operation: 'validateApiKey' }
            );
        }
    }

    protected getStartTime(): number {
        return Date.now();
    }

    protected createMetrics(startTime: number, tokenUsage: { input: number; output: number }): ExecutionMetrics {
        const endTime = Date.now();
        return {
            duration: endTime - startTime,
            startTime,
            endTime,
            tokenUsage: {
                input: tokenUsage.input,
                output: tokenUsage.output,
                total: tokenUsage.input + tokenUsage.output
            }
        };
    }

    protected validateRequest(request: LLMRequest): void {
        if (!request.messages || !Array.isArray(request.messages)) {
            throw new ValidationError(
                'Invalid request: messages must be an array',
                { request, messageType: typeof request.messages },
                { component: 'BaseLLMProvider', operation: 'validateRequest' }
            );
        }

        if (request.messages.length === 0) {
            throw new ValidationError(
                'Invalid request: messages array cannot be empty',
                { request },
                { component: 'BaseLLMProvider', operation: 'validateRequest' }
            );
        }

        if (request.functions && !this.supportsFunctions) {
            throw new LLMError(
                ErrorCode.LLM_API_ERROR,
                `${this.name} provider does not support function calling`,
                { 
                    provider: this.name, 
                    supportsFunctions: this.supportsFunctions,
                    functionsRequested: request.functions.length
                },
                { component: 'BaseLLMProvider', operation: 'validateRequest' }
            );
        }

        request.messages.forEach((message, index) => {
            if (!message.role || !message.content) {
                throw new ValidationError(
                    `Invalid message at index ${index}: must have role and content`,
                    { 
                        message, 
                        index, 
                        hasRole: !!message.role, 
                        hasContent: !!message.content 
                    },
                    { component: 'BaseLLMProvider', operation: 'validateRequest' }
                );
            }
        });

        logger.debug(LogCategory.AI, 'Request validation passed', {
            metadata: {
                provider: this.name,
                messageCount: request.messages.length,
                hasFunctions: !!request.functions
            }
        });
    }

    protected async handleError(error: any): Promise<never> {
        logger.error(LogCategory.AI, `${this.name} provider error`, {
            metadata: {
                error: error.message,
                stack: error.stack
            }
        });

        // If it's already a SymphonyError, just re-throw it
        if (error.code && error.component) {
            throw error;
        }

        // Convert generic errors to LLMError
        throw new LLMError(
            ErrorCode.LLM_API_ERROR,
            `${this.name} provider error: ${error.message}`,
            error,
            { component: 'BaseLLMProvider', operation: 'handleError' }
        );
    }
} 
```

## File: /Users/deepsaint/Desktop/symphony-sdk/src/llm/providers/openai.ts

```ts
import { logger, LogCategory } from '../../utils/logger';
import { LLMConfig, LLMRequest, LLMResponse, LLMRequestConfig } from '../types';
import OpenAI from 'openai';
import { BaseLLMProvider, ExtendedLLMConfig } from './base';
import { LLMError, ErrorCode } from '../../errors/index';

export class OpenAIProvider extends BaseLLMProvider {
    readonly name = 'openai';
    readonly supportsStreaming = true;
    readonly supportsFunctions = true;
    private client: OpenAI;

    constructor(config: LLMConfig) {
        // Convert to ExtendedLLMConfig with defaults
        const extendedConfig: ExtendedLLMConfig = {
            ...config,
            provider: 'openai',
            apiKey: config?.apiKey || process.env.OPENAI_API_KEY || '',
            model: config?.model || 'gpt-3.5-turbo',
            timeout: config?.timeout || 30000
        };

        // Check for API key early
        if (!extendedConfig.apiKey) {
            throw new LLMError(
                ErrorCode.MISSING_API_KEY,
                'OpenAI API key is not configured. Please set OPENAI_API_KEY environment variable or provide it in config.',
                { 
                    hasEnvKey: !!process.env.OPENAI_API_KEY,
                    hasConfigKey: !!config?.apiKey 
                },
                { component: 'OpenAIProvider', operation: 'constructor' }
            );
        }

        super(extendedConfig);

        this.client = new OpenAI({
            apiKey: extendedConfig.apiKey,
            timeout: extendedConfig.timeout || 30000,
        });
    }

    async initialize(): Promise<void> {
        this.validateApiKey();
        logger.info(LogCategory.AI, 'OpenAIProvider initialized', {
            model: this.config.model || 'gpt-3.5-turbo'
        });
    }

    async complete(request: LLMRequest, requestConfig?: LLMRequestConfig): Promise<LLMResponse> {
        try {
            this.validateRequest(request);
            
            const model = requestConfig?.model || this.config.model || 'gpt-3.5-turbo';
            const temperature = requestConfig?.temperature || this.config.temperature || 0.7;
            const maxTokens = requestConfig?.maxTokens || this.config.maxTokens;

            const openaiRequest: OpenAI.Chat.ChatCompletionCreateParams = {
                model,
                messages: request.messages as OpenAI.Chat.ChatCompletionMessageParam[],
                temperature,
                max_tokens: maxTokens,
                stream: false
            };

            // Add function support if provided
            if (request.functions && request.functions.length > 0) {
                if (!this.supportsFunctions) {
                    throw new LLMError(
                        ErrorCode.LLM_API_ERROR,
                        'This provider does not support function calling',
                        { provider: this.name, functions: request.functions },
                        { component: 'OpenAIProvider', operation: 'complete' }
                    );
                }
                
                openaiRequest.tools = request.functions.map(func => ({
                    type: 'function' as const,
                    function: {
                        name: func.name,
                        description: func.description,
                        parameters: func.parameters
                    }
                }));
                
                if (request.functionCall) {
                    if (request.functionCall === 'auto') {
                        openaiRequest.tool_choice = 'auto';
                    } else if (request.functionCall === 'none') {
                        openaiRequest.tool_choice = 'none';
                    } else if (typeof request.functionCall === 'object' && request.functionCall.name) {
                        openaiRequest.tool_choice = {
                            type: 'function',
                            function: { name: request.functionCall.name }
                        };
                    }
                }
            }

            const response = await this.client.chat.completions.create(openaiRequest);
            const choice = response.choices[0];
            const content = choice.message.content || '';

            return {
                content,
                model: model,
                role: 'assistant',
                usage: response.usage ? {
                    prompt_tokens: response.usage.prompt_tokens,
                    completion_tokens: response.usage.completion_tokens,
                    total_tokens: response.usage.total_tokens
                } : {
                    prompt_tokens: 0,
                    completion_tokens: 0,
                    total_tokens: 0
                },
                tool_calls: choice.message.tool_calls?.map(call => ({
                    id: call.id,
                    type: call.type,
                    function: call.function
                })),
                toString() {
                    return String(content);
                }
            };
        } catch (error: any) {
            throw this.handleError(error);
        }
    }

    async *completeStream(request: LLMRequest, requestConfig?: LLMRequestConfig): AsyncIterable<LLMResponse> {
        try {
            this.validateRequest(request);
            
            const model = requestConfig?.model || this.config.model || 'gpt-3.5-turbo';
            const temperature = requestConfig?.temperature || this.config.temperature || 0.7;
            const maxTokens = requestConfig?.maxTokens || this.config.maxTokens;

            const openaiRequest: OpenAI.Chat.ChatCompletionCreateParams = {
                model,
                messages: request.messages as OpenAI.Chat.ChatCompletionMessageParam[],
                temperature,
                max_tokens: maxTokens,
                stream: true
            };

            const apiKey = this.config.apiKey;
            if (!apiKey) {
                throw new LLMError(
                    ErrorCode.MISSING_API_KEY,
                    'OpenAI API key not found in provider config',
                    { hasConfigKey: !!this.config.apiKey },
                    { component: 'OpenAIProvider', operation: 'completeStream' }
                );
            }

            const response = await fetch('https://api.openai.com/v1/chat/completions', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'Authorization': `Bearer ${apiKey}`,
                },
                body: JSON.stringify(openaiRequest),
            });

            if (!response.ok) {
                const errorText = await response.text();
                let errorJson;
                try {
                    errorJson = JSON.parse(errorText);
                } catch {
                    throw new LLMError(
                        ErrorCode.LLM_API_ERROR,
                        `OpenAI API streaming error (${response.status}): ${errorText}`,
                        { status: response.status, responseText: errorText },
                        { component: 'OpenAIProvider', operation: 'completeStream' }
                    );
                }
                throw new LLMError(
                    response.status === 429 ? ErrorCode.LLM_RATE_LIMITED : ErrorCode.LLM_API_ERROR,
                    `OpenAI API streaming error (${response.status}): ${errorJson.error?.message || errorText}`,
                    { status: response.status, error: errorJson },
                    { component: 'OpenAIProvider', operation: 'completeStream' }
                );
            }

            if (!response.body) {
                throw new LLMError(
                    ErrorCode.LLM_API_ERROR,
                    'No response body received for stream',
                    { status: response.status },
                    { component: 'OpenAIProvider', operation: 'completeStream' }
                );
            }

            const reader = response.body.getReader();
            const decoder = new TextDecoder();

            try {
                while (true) {
                    const { done, value } = await reader.read();
                    if (done) break;

                    const chunk = decoder.decode(value);
                    const lines = chunk.split('\n');

                    for (const line of lines) {
                        if (line.startsWith('data: ')) {
                            const data = line.slice(6);
                            if (data === '[DONE]') continue;

                            try {
                                const parsed = JSON.parse(data);
                                const choice = parsed.choices?.[0];
                                if (choice) {
                                    const deltaContent = choice.delta?.content || '';
                                    yield {
                                        content: deltaContent,
                                        model: model,
                                        role: 'assistant',
                                        usage: parsed.usage ? {
                                            prompt_tokens: parsed.usage.prompt_tokens,
                                            completion_tokens: parsed.usage.completion_tokens,
                                            total_tokens: parsed.usage.total_tokens
                                        } : {
                                            prompt_tokens: 0,
                                            completion_tokens: 0,
                                            total_tokens: 0
                                        },
                                        toString() {
                                            return String(deltaContent);
                                        }
                                    };
                                }
                            } catch (parseError) {
                                // Skip malformed chunks
                                continue;
                            }
                        }
                    }
                }
            } finally {
                reader.releaseLock();
            }
        } catch (error: any) {
            throw this.handleError(error);
        }
    }

    protected handleError(error: any): never {
        logger.error(LogCategory.AI, `${this.name} provider error`, {
            metadata: {
                error: error.message,
                stack: error.stack
            }
        });

        // Convert OpenAI-specific errors to SymphonyErrors
        if (error.status === 401) {
            throw new LLMError(
                ErrorCode.MISSING_API_KEY,
                'OpenAI API authentication failed - invalid API key',
                error,
                { component: 'OpenAIProvider', operation: 'handleError' }
            );
        }

        if (error.status === 429) {
            throw new LLMError(
                ErrorCode.LLM_RATE_LIMITED,
                'OpenAI API rate limit exceeded',
                error,
                { component: 'OpenAIProvider', operation: 'handleError' }
            );
        }

        if (error.status === 402 || error.message?.includes('quota')) {
            throw new LLMError(
                ErrorCode.LLM_QUOTA_EXCEEDED,
                'OpenAI API quota exceeded',
                error,
                { component: 'OpenAIProvider', operation: 'handleError' }
            );
        }

        if (error.status >= 400 && error.status < 500) {
            throw new LLMError(
                ErrorCode.LLM_INVALID_RESPONSE,
                `OpenAI API client error: ${error.message}`,
                error,
                { component: 'OpenAIProvider', operation: 'handleError' }
            );
        }

        if (error.status >= 500) {
            throw new LLMError(
                ErrorCode.LLM_API_ERROR,
                `OpenAI API server error: ${error.message}`,
                error,
                { component: 'OpenAIProvider', operation: 'handleError' }
            );
        }

        // Generic fallback
        throw new LLMError(
            ErrorCode.LLM_API_ERROR,
            `OpenAI API error: ${error.message}`,
            error,
            { component: 'OpenAIProvider', operation: 'handleError' }
        );
    }
}
```

## File: /Users/deepsaint/Desktop/symphony-sdk/src/llm/types.ts

```ts
import { ExecutionMetrics } from '../types/sdk';

// Base configuration that requires API key
export interface LLMConfig {
    provider: 'openai' | 'anthropic' | 'google';
    apiKey: string;  // Required but always sourced from environment
    model?: string;
    temperature?: number;
    maxTokens?: number;
    timeout?: number;
}

// Configuration that can be passed in requests (no API key)
export interface LLMRequestConfig {
    model?: string;
    temperature?: number;
    maxTokens?: number;
    timeout?: number;
}

export interface LLMMessage {
    role: 'system' | 'user' | 'assistant' | 'function' | 'tool';
    content: string | null;
    name?: string; // Used for role: 'function' (legacy) AND role: 'tool' (name of the function that was called)
    tool_call_id?: string; // Used for role: 'tool'
    tool_calls?: {
        id: string;
        type: 'function';
        function: {
            name: string;
            arguments: string;
        };
    }[];
}

export interface LLMFunctionDefinition {
    name: string;
    description: string;
    parameters: {
        type: 'object';
        properties: Record<string, any>;
        required?: string[];
    };
}

export interface LLMRequest {
    messages: LLMMessage[];
    functions?: LLMFunctionDefinition[];
    functionCall?: {
        name: string;
    } | "auto" | "none";
    tool_choice?: "none" | "auto" | { type: "function"; function: { name: string } };
    response_format?: { type: "text" | "json_object" };
    temperature?: number;
    maxTokens?: number;
    stream?: boolean;
    provider?: string;
    llmConfig?: LLMRequestConfig;  // Only non-sensitive configuration
    expectsJsonResponse?: boolean; // ADDED: Hint that the caller expects a JSON-structured response
}

export interface LLMResponse {
    content: string | null;
    model: string;
    role: 'assistant';
    functionCall?: {
        id?: string;
        name: string;
        arguments: string;
    };
    tool_calls?: {
        id: string;
        type: 'function';
        function: {
            name: string;
            arguments: string;
        };
    }[];
    usage: {
        prompt_tokens: number;
        completion_tokens: number;
        total_tokens: number;
    };
    metrics?: ExecutionMetrics & {
        tokenUsage: {
            input: number;
            output: number;
            total: number;
        };
    };
    toString(): string;
}

export interface LLMProvider {
    name: string;
    supportsStreaming: boolean;
    initialize(): Promise<void>;
    complete(request: LLMRequest, configOverride?: LLMRequestConfig): Promise<LLMResponse>;
    completeStream(request: LLMRequest, configOverride?: LLMRequestConfig): AsyncIterable<LLMResponse>;
} 
```

## File: /Users/deepsaint/Desktop/symphony-sdk/src/llm/handler.ts

```ts
import { 
    LLMProvider, 
    LLMConfig, 
    LLMRequest, 
    LLMResponse,
    LLMRequestConfig
} from './types';
import { OpenAIProvider } from './providers/openai';
import { logger, LogCategory } from '../utils/logger';
import { envConfig } from '../utils/env';
import { LLMError, ConfigurationError, ErrorCode } from '../errors/index';

export class LLMHandler {
    private static instance: LLMHandler;
    private providers: Map<string, LLMProvider>;
    private defaultProvider?: string;
    private cacheService?: any; // ICacheService from Symphony

    private constructor() {
        this.providers = new Map();
        this.initializeDefaultProviders().catch(error => {
            logger.error(LogCategory.AI, 'Failed to initialize default providers', {
                metadata: {
                    error: error.message
                }
            });
        });
    }

    static getInstance(): LLMHandler {
        if (!LLMHandler.instance) {
            LLMHandler.instance = new LLMHandler();
        }
        return LLMHandler.instance;
    }

    // Set cache service from Symphony
    setCacheService(cacheService: any): void {
        this.cacheService = cacheService;
    }

    // Get cache service for providers
    getCacheService(): any {
        return this.cacheService;
    }

    private async initializeDefaultProviders(): Promise<void> {
        // Initialize OpenAI if API key is provided
        if (envConfig.openaiApiKey) {
            logger.info(LogCategory.AI, 'Initializing OpenAI provider with API key:', {
                metadata: {
                    apiKey: envConfig.openaiApiKey,
                    source: 'envConfig',
                    file: require.resolve('../utils/env')
                }
            });
            
            await this.registerProvider({
                provider: 'openai',
                model: envConfig.defaultModel,
                apiKey: envConfig.openaiApiKey,  // Always use env API key
                temperature: envConfig.defaultTemperature,
                maxTokens: envConfig.defaultMaxTokens,
                timeout: envConfig.requestTimeoutMs
            });
            // Always set OpenAI as default provider
            this.defaultProvider = 'openai';
            
            logger.info(LogCategory.AI, 'Provider registered successfully', {
                metadata: {
                    name: 'openai',
                    type: 'OpenAIProvider',
                    apiKey: envConfig.openaiApiKey
                }
            });
        } else {
            throw new LLMError(
                ErrorCode.MISSING_API_KEY,
                'OpenAI API key is required in environment configuration',
                { component: 'LLMHandler', operation: 'initializeDefaultProviders' }
            );
        }
    }

    async registerProvider(config: LLMConfig): Promise<void> {
        const providerName = config.provider?.toLowerCase();
        if (!providerName) {
            throw new ConfigurationError(
                'Provider name must be specified in LLMConfig',
                { config },
                { component: 'LLMHandler', operation: 'registerProvider' }
            );
        }

        // For now, only OpenAI is instantiated, extend this for other providers
        if (providerName !== 'openai') {
            logger.warn(LogCategory.AI, `Provider ${providerName} not fully supported for dynamic registration, only OpenAI for now.`);
            // Allow it to proceed if it's just updating an existing openai config
            if (providerName !== 'openai' && !this.providers.has(providerName)) {
                throw new ConfigurationError(
                    `Provider ${providerName} is not supported for dynamic registration yet`,
                    { providerName, supportedProviders: ['openai'] },
                    { component: 'LLMHandler', operation: 'registerProvider' }
                );
            }
        }

        try {
            let finalConfig = { ...config };
            // Ensure API key from environment for OpenAI for security, can be adapted for other providers
            if (providerName === 'openai') {
                if (!envConfig.openaiApiKey) {
                    throw new LLMError(
                        ErrorCode.MISSING_API_KEY,
                        'OpenAI API key not found in environment for registration',
                        { providerName },
                        { component: 'LLMHandler', operation: 'registerProvider' }
                    );
                }
                finalConfig.apiKey = envConfig.openaiApiKey;
            }
            // Add similar logic for other providers if they source API keys from env

            // Log what config is being used for provider registration
            logger.debug(LogCategory.AI, `Registering/Updating provider ${providerName} with config:`, {
                model: finalConfig.model,
                temperature: finalConfig.temperature,
                maxTokens: finalConfig.maxTokens,
                provider: finalConfig.provider
            });

            // Instantiate or update provider
            // This is simplified; a real multi-provider setup would have a factory or switch
            if (providerName === 'openai') {
                const providerInstance = new OpenAIProvider(finalConfig);
                this.providers.set(providerName, providerInstance);
            } else {
                // If other providers are pre-registered, this could update them
                // For now, this branch might not be hit if we throw error above for non-openai
                const existingProvider = this.providers.get(providerName);
                if (existingProvider) {
                    // Providers would need an updateConfig method for this to be clean
                    logger.warn(LogCategory.AI, `Updating existing provider ${providerName} via re-registration (not ideal).`);
                    // This creates a new instance, effectively replacing the old one.
                    // const updatedProvider = new WhatEverProvider(finalConfig);
                    // this.providers.set(providerName, updatedProvider);
                } else {
                    throw new ConfigurationError(
                        `No existing provider ${providerName} to update and dynamic creation not supported`,
                        { providerName, existingProviders: Array.from(this.providers.keys()) },
                        { component: 'LLMHandler', operation: 'registerProvider' }
                    );
                }
            }
            
            if (!this.defaultProvider) {
                this.defaultProvider = providerName;
            }

            logger.info(LogCategory.AI, `Provider ${providerName} (re)registered successfully`);
        } catch (error: any) {
            logger.error(LogCategory.AI, 'Failed to register provider', { 
                providerName, 
                error: error.message 
            });
            throw error;
        }
    }

    getProvider(name?: string): LLMProvider {
        const providerName = name?.toLowerCase() || this.defaultProvider;
        if (!providerName) {
            throw new ConfigurationError(
                'No default provider set',
                { availableProviders: Array.from(this.providers.keys()) },
                { component: 'LLMHandler', operation: 'getProvider' }
            );
        }

        const provider = this.providers.get(providerName);
        if (!provider) {
            throw new LLMError(
                ErrorCode.LLM_API_ERROR,
                `Provider not found: ${providerName}`,
                { 
                    requestedProvider: providerName,
                    availableProviders: Array.from(this.providers.keys())
                },
                { component: 'LLMHandler', operation: 'getProvider' }
            );
        }

        return provider;
    }

    async complete(request: LLMRequest): Promise<LLMResponse> {
        const targetProviderName = request.provider?.toLowerCase() || this.defaultProvider;
        if (!targetProviderName) {
            throw new ConfigurationError(
                'No provider specified in request and no default provider set',
                { request, availableProviders: Array.from(this.providers.keys()) },
                { component: 'LLMHandler', operation: 'complete' }
            );
        }

        let providerInstance = this.providers.get(targetProviderName);

        if (!providerInstance) {
            // Attempt to initialize if it's a known type (e.g. openai) and config implies it
            if (targetProviderName === 'openai' && envConfig.openaiApiKey) {
                logger.warn(LogCategory.AI, `OpenAI provider for ${targetProviderName} not found, attempting on-demand initialization.`);
                await this.registerProvider({
                    provider: 'openai',
                    apiKey: envConfig.openaiApiKey,
                    model: request.llmConfig?.model || envConfig.defaultModel,
                    temperature: request.llmConfig?.temperature,
                    maxTokens: request.llmConfig?.maxTokens,
                });
                providerInstance = this.providers.get(targetProviderName);
                if (!providerInstance) {
                    throw new LLMError(
                        ErrorCode.LLM_API_ERROR,
                        `Failed to dynamically initialize provider: ${targetProviderName}`,
                        { targetProviderName, request },
                        { component: 'LLMHandler', operation: 'complete' }
                    );
                }
            } else {
                throw new LLMError(
                    ErrorCode.LLM_API_ERROR,
                    `Provider ${targetProviderName} not found or not configured for on-demand initialization`,
                    { 
                        targetProviderName, 
                        availableProviders: Array.from(this.providers.keys()),
                        hasOpenAIKey: !!envConfig.openaiApiKey
                    },
                    { component: 'LLMHandler', operation: 'complete' }
                );
            }
        }
        
        return providerInstance.complete(request, request.llmConfig);
    }

    async inference(prompt: string, llmConfig?: LLMRequestConfig): Promise<string> {
        const request: LLMRequest = {
            messages: [{ role: 'user', content: prompt }],
            llmConfig: llmConfig
        };
        const response = await this.complete(request);
        return response.content || '';
    }

    async *completeStream(
        request: LLMRequest, 
        providerName?: string // providerName from argument is less common, request.provider takes precedence
    ): AsyncGenerator<LLMResponse> {
        const targetProviderName = request.provider?.toLowerCase() || providerName?.toLowerCase() || this.defaultProvider;
        if (!targetProviderName) {
            throw new ConfigurationError(
                'No provider specified in request or argument, and no default provider set',
                { request, providerName, availableProviders: Array.from(this.providers.keys()) },
                { component: 'LLMHandler', operation: 'completeStream' }
            );
        }

        let providerInstance = this.providers.get(targetProviderName);

        if (!providerInstance) {
            if (targetProviderName === 'openai' && envConfig.openaiApiKey) {
                logger.warn(LogCategory.AI, `OpenAI provider for ${targetProviderName} not found in completeStream, attempting on-demand initialization.`);
                await this.registerProvider({
                    provider: 'openai',
                    apiKey: envConfig.openaiApiKey,
                    model: request.llmConfig?.model || envConfig.defaultModel,
                    temperature: request.llmConfig?.temperature,
                    maxTokens: request.llmConfig?.maxTokens,
                });
                providerInstance = this.providers.get(targetProviderName);
                if (!providerInstance) {
                    throw new LLMError(
                        ErrorCode.LLM_API_ERROR,
                        `Failed to dynamically initialize provider for stream: ${targetProviderName}`,
                        { targetProviderName, request },
                        { component: 'LLMHandler', operation: 'completeStream' }
                    );
                }
            } else {
                throw new LLMError(
                    ErrorCode.LLM_API_ERROR,
                    `Provider ${targetProviderName} not found or not configured for on-demand initialization for stream`,
                    { 
                        targetProviderName, 
                        availableProviders: Array.from(this.providers.keys()),
                        hasOpenAIKey: !!envConfig.openaiApiKey
                    },
                    { component: 'LLMHandler', operation: 'completeStream' }
                );
            }
        }
        
        if (!providerInstance.supportsStreaming) {
            // Fallback to non-streaming
            const response = await providerInstance.complete(request, request.llmConfig);
            yield response;
            return;
        }

        yield* providerInstance.completeStream(request, request.llmConfig);
    }
} 
```



---

> ðŸ“¸ Generated with [Jockey CLI](https://github.com/saint0x/jockey-cli)
